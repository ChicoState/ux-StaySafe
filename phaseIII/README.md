# Phase III: Prototypes and User Testing

## Introduction

Stay Safe addresses the need for individuals to stay informed and aware of criminal activity in their surroundings. Whether one is concerned about the safety of their own neighborhood, considering a new place to live, planning a trip to an unfamiliar area, or even evaluating a potential real estate investment. Our focus on this portion of our project has been on building and testing a prototype to see how our work and research has been so far. We’ve taken our feedback on our wireframes and applied it to our prototype, as well as creating a more professional look for the project, rather than just placeholders everywhere. With this prototype, we conducted tests with potential users, and gathered up any notes to help us understand what we’ve done well and what still needs improvement.

## Methods


For our research, we created a [test protocol](x18 StaySafe Test Protocol Script.pdf) that intended to explore the ins and outs of the prototype, as well as gathering information and thoughts from our testers. For our protocol, we had 6 testers. We’d clarify what our test was about to the user, and try to get an idea of how knowledgeable they were on the subject. We asked if they’d used any crime apps before, and if they had a preferred one, as well as what they’d want to see from a crime app. Then, we provided the users with 5 tasks to complete, wanting them to achieve some sort of goal within our prototype. These tasks would intend to explore various different aspects of the prototype, to see how intuitive and clear each aspect of our system would be. Before we could do any of this however, we created a [consent form](INFORMED CONSENT FORM.pdf) that went over what we were researching, and required written consent before continuing onto the tests. With this form, we made it clear that no personal information would be saved, that we could stop the test at any point if they wished, and our stored data would be cleared after the semester finished.

We had our users think-aloud, to get an idea of how a user might think while using our app, since they don’t know the full functionality of the app like the team members do. We’d check to see if they could finish the task, and ask them to rate how difficult it was on a scale from 1-5, with 1 being the hardest and 5 being the easiest. Then, we’d gather any extra thoughts or notes about the task after their completion. We’d repeat this process for each task, ending the session with a debrief with them. The debrief served to gather any remaining feedback they had, asking what they liked and disliked about the prototype, what confusing or unexpected things they came across, features that they expected but weren’t there, and their thoughts on what could be improved. Everything done in this section of the project served to check what we’ve done well with our prototype, and what could be improved in the future. Tests like these are vital to ensuring that you’ve created the best possible project that you can.

## Findings


From each of our tasks, we collected data on a few different things. Whether or not the user completed the test, what score of difficulty they gave it, and extra notes. Across our 5 tests, 4 of them had a 100% completion rate, with one task getting a 66% completion rate, 4 managing to complete the test with the 2 others failing. The users who failed got a bit lost trying to navigate through the menus to the correct location. Once they’d finished their assigned task, we’d get a difficulty rating from them on a scale from 1-5. Users found most of the tasks easy to complete, with 4 out of 5 and 5 out of 5 being the most common scores, with occasional outliers. However, the task with a 66% completion rate stood out, receiving difficulty scores of 2 and 3, reflecting the challenges users faced.

Feedback we received was overall pretty positive, with a handful of notes on smaller things that could be improved. Whenever there was a dip in score or completion rates, the issue that users had tended to be the same, which helps us know if one experience was just a fluke or not. Trends with our quantitative data showed that generally, tasks were successfully completed and rated as “easy to complete”, with a couple exceptions here and there. When there was confusion about something, it tended to lead to a lower score. As for qualitative trends, we had received praise for a few different features and visual clarity of some areas, with a few key areas we could improve on, such as changing our “starred” reports section to “favorites” or “important” instead, alert and icons not having expected interactivity, and some unclear UI elements for example. Some of these complaints were due to limitations with the prototype, so some areas are only a problem due to the project’s temporary limits. Following our tests, our debriefs conducted at the end of each test helped us cultivate a handful of answers and suggestions we could look to for potential improvements.

Our full notes are available in a [spreadsheet](spreadsheet_merged.pdf) that we created for this assignment.


## Conclusions



Throughout the process, we identified from users what features they enjoyed and what parts needed improvement due to confusion or needed more modifications. This phase of the project emphasized on perfecting the design and transitions on the prototype so that it worked smoothly as a potential app. Then we got feedback from users that may or may not have had experience with a crime alert app. Looking back on the ratings we received from our research, generally most people understood and were able to navigate their way through the prototype while a select few got confused with the functionality of the app. After carefully reviewing the notes we took during the research process, we went back to the prototype app to understand why users felt confused or lost on certain tasks. Some recommendations made by users were to make the delete option more user friendly on the starred reports or even how to navigate to the starred report page. Another critique was to provide the user the option of alerting their friends/family easier on the map. Finally, another suggestion that was raised was having the notification display page more clearer as it was confusing to navigate to certain features. With all this feedback, I’m sure that future users will experience a near flawless prototype if given the chance.

Our [prototype](https://www.figma.com/proto/IM5jBCK9K5tjQa3mwCc5cH/User-Scenario-Wireframe?node-id=98-2919&t=VIT6COEQUJwPKgJA-1) is available as well to view, to see how we did everything. We weren’t able to implement feedback from our tests, so the version we have was finished right before the user tests.


## Caveats

Some limitations affected the potential findings and feedback we could gather. First of all, the users testing our prototype were students that have a strong background in technology. Going forward, getting feedback from users with different backgrounds would prove to be more beneficial in our research to find if StaySafe is user friendly. For example, having a wider range of ages and different experiences using technology would benefit the research process even more. With all our users testing our prototype, we found almost all have never really properly used a crime alert app. Additionally, we were able to gather three users to test our prototype while some groups were only able to get two due to not enough people signing up. We definitely were lucky as it provided a wider range of responses given back to us by the users that we did not expect.


